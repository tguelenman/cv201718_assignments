{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übung 6: Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuronal Netze benötigen oft eine große Menge an Trainingsdaten, damit es nicht zu overfitting kommt. Transfer learning erlaubt es, mit relativ geringen Datenmenge dennoch erfolgreiche große Netze zu trainieren. Dabei verwendet man ein bereits auf einen anderen Datensatz (z.b. ImageNet) vortrainiertes Netzwerk, und ersetzt nur das letzte Layer durch ein neues. In dieser Übung geht es darum, ein Netzwerk für die Erkennung von Geparden und Leoparden in der freien Wildbahn zu trainineren. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten laden\n",
    "\n",
    "Lade die Daten hier herunter: http://tonic.imp.fu-berlin.de/cv_data/data.tar.gz\n",
    "\n",
    "Die Daten wurde bereits in ein Trainings- und Validierungsset geteilt. Die Ordnerstruktur ist wie bei vielen Bildklassifierungsdatensetzen so aufgebaut. Es gibt zwei Unterordner für die Trainings- und Validierunsdaten. In diesen Ordnern liegen dann jeweils alle Bilder von einer Klasse in einem Unterordner mit dem Namen der Klasse.\n",
    "\n",
    "Ein Beispiel: Die Trainingsbilder für die Klasse \"cheetah\" liegen in dem Unterordner train/cheetah\n",
    "\n",
    "Diese Orderstruktur wird auch von dem in keras enhaltenen ImageDataGenerator unterstützt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session, get_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "image_input_size = (224, 224)\n",
    "data_path = './data/'\n",
    "\n",
    "#geaddet\n",
    "prob_drop_conv = 0.5                # drop probability for dropout @ conv layer\n",
    "pool_size = (2, 2)                  # size of pooling area for max pooling\n",
    "nb_epoch = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17857 images belonging to 3 classes.\n",
      "Found 1915 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_path = os.path.join(data_path, 'train')\n",
    "val_data_path = os.path.join(data_path, 'val')\n",
    "\n",
    "izw_classes = ('unknown', 'cheetah', 'leopard')\n",
    "\n",
    "generator = ImageDataGenerator(horizontal_flip=True)\n",
    "val_generator = ImageDataGenerator(horizontal_flip=False)\n",
    "\n",
    "train_gen = generator.flow_from_directory(\n",
    "    train_data_path, \n",
    "    target_size=image_input_size,\n",
    "    classes=izw_classes,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_gen = val_generator.flow_from_directory(\n",
    "    val_data_path, \n",
    "    target_size=image_input_size,\n",
    "    classes=izw_classes,\n",
    "    batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ohne transfer learning\n",
    "\n",
    "Trainiere zuerst ein kleines Classifer-Netzwerk ohne transfer learning. Falls du keine Grafikkarte hast, solltest du nicht die volle Auflösung (siehe Variable image_input_size) verwenden, da das Training sonst zu lange dauert. Eine Bildgröße von 32x32 Pixeln wäre zum Beispiel möglich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape, name=None):\n",
    "    return initializers.normal(shape, scale=0.01, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), input_shape=[224, 224,..., padding=\"same\", activation=\"relu\")`\n",
      "  \n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(strides=(2, 2), padding=\"same\", pool_size=(2, 2))`\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  del sys.path[0]\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(strides=(2, 2), padding=\"same\", pool_size=(2, 2))`\n",
      "  \n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(strides=(2, 2), padding=\"same\", pool_size=(2, 2))`\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., verbose=1, epochs=30, shuffle=True)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 224, 224, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 112, 112, 16)      1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 56, 56, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 56, 56, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 56, 56, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 625)               15680625  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 625)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 1878      \n",
      "=================================================================\n",
      "Total params: 15,688,535\n",
      "Trainable params: 15,688,535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "559/559 [==============================] - 114s 203ms/step - loss: 5.7191 - acc: 0.6451\n",
      "Epoch 2/30\n",
      "559/559 [==============================] - 115s 206ms/step - loss: 5.7073 - acc: 0.6459\n",
      "Epoch 3/30\n",
      "559/559 [==============================] - 122s 219ms/step - loss: 5.6785 - acc: 0.6477\n",
      "Epoch 4/30\n",
      "559/559 [==============================] - 142s 254ms/step - loss: 5.6794 - acc: 0.6476\n",
      "Epoch 5/30\n",
      "559/559 [==============================] - 149s 266ms/step - loss: 5.6794 - acc: 0.6476\n",
      "Epoch 6/30\n",
      "559/559 [==============================] - 141s 253ms/step - loss: 5.6794 - acc: 0.6476\n",
      "Epoch 7/30\n",
      "559/559 [==============================] - 147s 262ms/step - loss: 6.4014 - acc: 0.6028\n",
      "Epoch 8/30\n",
      "559/559 [==============================] - 151s 270ms/step - loss: 5.6812 - acc: 0.6475\n",
      "Epoch 9/30\n",
      "559/559 [==============================] - 148s 264ms/step - loss: 5.7091 - acc: 0.6458\n",
      "Epoch 10/30\n",
      "559/559 [==============================] - 143s 256ms/step - loss: 5.6803 - acc: 0.6476\n",
      "Epoch 11/30\n",
      "559/559 [==============================] - 138s 247ms/step - loss: 5.7002 - acc: 0.6463\n",
      "Epoch 12/30\n",
      "559/559 [==============================] - 125s 224ms/step - loss: 5.6794 - acc: 0.6476\n",
      "Epoch 13/30\n",
      "559/559 [==============================] - 127s 227ms/step - loss: 5.7073 - acc: 0.6459\n",
      "Epoch 14/30\n",
      "559/559 [==============================] - 129s 231ms/step - loss: 5.6785 - acc: 0.6477\n",
      "Epoch 15/30\n",
      "559/559 [==============================] - 130s 233ms/step - loss: 5.6803 - acc: 0.6476\n",
      "Epoch 16/30\n",
      "559/559 [==============================] - 130s 232ms/step - loss: 5.6794 - acc: 0.6476\n",
      "Epoch 17/30\n",
      "559/559 [==============================] - 131s 234ms/step - loss: 5.6803 - acc: 0.6476\n",
      "Epoch 18/30\n",
      "559/559 [==============================] - 134s 240ms/step - loss: 5.6803 - acc: 0.6476\n",
      "Epoch 19/30\n",
      "559/559 [==============================] - 126s 225ms/step - loss: 5.7073 - acc: 0.6459\n",
      "Epoch 20/30\n",
      "559/559 [==============================] - 130s 232ms/step - loss: 5.6785 - acc: 0.6477\n",
      "Epoch 21/30\n",
      "559/559 [==============================] - 139s 249ms/step - loss: 5.6803 - acc: 0.6476\n",
      "Epoch 22/30\n",
      "559/559 [==============================] - 139s 249ms/step - loss: 5.7082 - acc: 0.6459\n",
      "Epoch 23/30\n",
      "559/559 [==============================] - 137s 245ms/step - loss: 5.7073 - acc: 0.6459\n",
      "Epoch 24/30\n",
      "559/559 [==============================] - 135s 242ms/step - loss: 5.6803 - acc: 0.6476\n",
      "Epoch 25/30\n",
      "559/559 [==============================] - 138s 248ms/step - loss: 5.6803 - acc: 0.6476\n",
      "Epoch 26/30\n",
      "559/559 [==============================] - 139s 248ms/step - loss: 5.7073 - acc: 0.6459\n",
      "Epoch 27/30\n",
      "559/559 [==============================] - 137s 246ms/step - loss: 5.7073 - acc: 0.6459\n",
      "Epoch 28/30\n",
      "559/559 [==============================] - 137s 245ms/step - loss: 5.6794 - acc: 0.6476\n",
      "Epoch 29/30\n",
      "559/559 [==============================] - 138s 247ms/step - loss: 5.6794 - acc: 0.6476\n",
      "Epoch 30/30\n",
      "559/559 [==============================] - 136s 243ms/step - loss: 5.7073 - acc: 0.6459\n",
      "Summary: Loss over the test dataset: 2.01, Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#das convolutional net tutorial von keras\n",
    "\n",
    "# Convolutional model\n",
    "model = Sequential()\n",
    "\n",
    "# conv1 layer\n",
    "model.add(Convolution2D(8, 3, 3, border_mode='same', activation='relu', input_shape=[224,224,3]))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), border_mode='same'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "# conv2 layer\n",
    "model.add(Convolution2D(16, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), border_mode='same'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "# conv3 layer\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "# fc1 layer\n",
    "model.add(Dense(625, activation='relu'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "# fc2 layer\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "opt = RMSprop(lr=0.001, rho=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "history = model.fit_generator(train_gen, nb_epoch=nb_epoch, shuffle=True, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "#evaluation = model.evaluate_generator(val_gen, verbose=1)\n",
    "evaluation = model.evaluate_generator(val_gen, 1)\n",
    "\n",
    "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstelle eine Confusion matrix basierend auf den Ausgaben des Netzes für die Validierungsdaten und berechne den ROC AUC für die Klasse cheetah. Du kannst hierfür optional die scikit-learn Bibliothek verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained network\n",
    "\n",
    "Lade nun ein auf Imagenet vortrainiertes Netzwerk und klassifiziere damit die Validierungsdaten. Eine Anleitung für keras findest du hier: https://keras.io/applications\n",
    "\n",
    "Du kannst selber entscheiden, welche Netzwerkarchitektur du verwendest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da der ImageNet-Datensatz auch die Klassen cheetah und leopard enthält, können wir sogar ohne transfer learning das vortrainierte Netzwerk evaluieren. Interpretiere alle Klassen außer cheetah und leopard als unknown und berechne wie im vorherigen Schritt die Confusion matrix und den ROC AUC score für die Klasse cheetah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "Das vortrainierte Netzwerk kann nun mit unseren Daten weitertrainiert werden. Ersetze dafür das letzte Layer in dem Netzwerk mit einem Dense Layer mit 3 Ausgaben für unsere Klassen cheetah, leopard und unknown. Du kannst selbst entscheiden, ob du nun das komplette Netzwerk mit trainierst oder nur das neu eingefügte, letzte Layer.\n",
    "\n",
    "Auch hierfür kannst du dich wieder an der keras Anleitung orientieren: https://keras.io/applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluiere das so trainierte Netzwerk wie in den letzten beiden Aufgaben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswertung\n",
    "\n",
    "Beschreibe kurz qualitativ die Resultate. Wie unterscheiden sich die trainierten Netzwerke, zum Beispiel im Bezug auf die Genauigkeit oder die Laufzeit? Welche Entscheidungen musstest du bei der Erfüllung der Aufgaben treffen und warum hast du dich für den von dir gewählten Weg entschieden?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
