{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Semantic Segmentation\n",
    "Heute werden wir einfache Netzwerkarchitekturen für \"Semantic Segmentation\" testen. Ziel ist es dieses Paper in den Grundzügen zu implementieren: https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf. Bitte lesen!\n",
    "\n",
    "## Daten\n",
    "\n",
    "Es gibt einige gute Datensätze, die ihr (bei gegebener Hardware) herunterladen und benutzen könnt. Für diejenigen, die auf CPUs rechnen gilt immer der Tip: Bilder downsamplen!\n",
    "\n",
    "Sucht Euch einen Satensatz aus: \n",
    "\n",
    "KITTI: http://www.cvlibs.net/download.php?file=data_semantics.zip (~300 MB, 200 Bilder)\n",
    "\n",
    "DUS: http://www.6d-vision.com/scene-labeling (~3 GB, 500 Bilder)\n",
    "\n",
    "MIT. http://sceneparsing.csail.mit.edu/ (~1 GB, links siehe auf Seite)\n",
    "\n",
    "## Exc. 7.1 Fully convolutional network, no downsampling\n",
    "Implementiere die in der Vorlesung besprochene Netzwerkarchitektur von aufeinanderfolgenden CONV-Schichten (stride=1, mit zero-padding), um eine Ausgabeschicht zu bekommen, die die Eingabegröße aufweist. Tip: die letzte CONV-Schicht sollte eine Tiefe haben, die zur Zahl der Klassen korrespondiert. Benutze den L2-Loss zum Labelbild (Achtung: ihr müsst dafür entweder das Labelbild oder den Ausgabetensor umformulieren).\n",
    "\n",
    "Trainiere das Netzwerk auf den von Dir gewählten Datensatz und zeige den Verlauf des Losses, und einige zufällig gewählte Beispielbilder mit ihren vorhergesagten Segmentierungen an. (**RESULT**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session, get_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D, Dropout, UpSampling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.applications import resnet50\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Hi Tim, vorrangig aus GPU-Gründen haben wir die Jan(n)is-Gruppe bei dieser Übung als auch bei der nächsten mit ins Boot geholt.\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(\"Hi Tim, vorrangig aus GPU-Gründen haben wir die Jan(n)is-Gruppe bei dieser Übung als auch bei der nächsten mit ins Boot geholt.\")\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 1 classes.\n",
      "Found 200 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_generator = ImageDataGenerator()\n",
    "label_generator = ImageDataGenerator()\n",
    "\n",
    "# Beispiel für den KITTI-Datensatz. Ich habe die 200 training samples in 180 train- und 20 testbilder\n",
    "# geteilt (macht 180 samples inkl. labels)\n",
    "# um uns das Leben leichter zu machen, Bilder heruntersamplen\n",
    "img_size = (40, 128)\n",
    "\n",
    "\n",
    "# Bild- und Label-Generator\n",
    "Q1 = image_generator.flow_from_directory( './data_semantics/training/images',\n",
    "                                        class_mode = None,\n",
    "                                        batch_size=1,\n",
    "                                        target_size=img_size, seed=1)\n",
    "\n",
    "Q2 = label_generator.flow_from_directory( './data_semantics/training/labels',\n",
    "                                        color_mode = 'grayscale',\n",
    "                                        class_mode = None,\n",
    "                                        batch_size=1,\n",
    "                                        target_size=img_size, seed=1)\n",
    "\n",
    "# ... kombinieren\n",
    "train_generator = zip(Q1, Q2)\n",
    "\n",
    "# mach der Definition des Modells:\n",
    "# model.fit_generator( train_generator, steps_per_epoch = 1000, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), padding=\"same\", activation=\"relu\", input_shape=[40, 128, ...)`\n",
      "  \n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "  if sys.path[0] == '':\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\t\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (3, 3), padding=\"same\", activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_275 (Conv2D)          (None, 40, 128, 4)        112       \n",
      "_________________________________________________________________\n",
      "dropout_252 (Dropout)        (None, 40, 128, 4)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_276 (Conv2D)          (None, 40, 128, 4)        148       \n",
      "_________________________________________________________________\n",
      "dropout_253 (Dropout)        (None, 40, 128, 4)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_277 (Conv2D)          (None, 40, 128, 8)        296       \n",
      "_________________________________________________________________\n",
      "dropout_254 (Dropout)        (None, 40, 128, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_278 (Conv2D)          (None, 40, 128, 8)        584       \n",
      "_________________________________________________________________\n",
      "dropout_255 (Dropout)        (None, 40, 128, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_279 (Conv2D)          (None, 40, 128, 8)        584       \n",
      "_________________________________________________________________\n",
      "dropout_256 (Dropout)        (None, 40, 128, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_280 (Conv2D)          (None, 40, 128, 16)       1168      \n",
      "_________________________________________________________________\n",
      "dropout_257 (Dropout)        (None, 40, 128, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_281 (Conv2D)          (None, 40, 128, 16)       2320      \n",
      "_________________________________________________________________\n",
      "dropout_258 (Dropout)        (None, 40, 128, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_282 (Conv2D)          (None, 40, 128, 8)        1160      \n",
      "_________________________________________________________________\n",
      "dropout_259 (Dropout)        (None, 40, 128, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_283 (Conv2D)          (None, 40, 128, 8)        584       \n",
      "_________________________________________________________________\n",
      "dropout_260 (Dropout)        (None, 40, 128, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_284 (Conv2D)          (None, 40, 128, 8)        584       \n",
      "_________________________________________________________________\n",
      "dropout_261 (Dropout)        (None, 40, 128, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_285 (Conv2D)          (None, 40, 128, 4)        292       \n",
      "_________________________________________________________________\n",
      "dropout_262 (Dropout)        (None, 40, 128, 4)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_286 (Conv2D)          (None, 40, 128, 1)        37        \n",
      "=================================================================\n",
      "Total params: 7,869\n",
      "Trainable params: 7,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prob_drop_conv = 0.2\n",
    "opt = RMSprop(lr=0.01, rho=0.9)\n",
    "\n",
    "model = Sequential()\n",
    "#wir haben zuerst die Struktur gebaut mit Up und Downsampling und das dann nur auskommentiert\n",
    "#funktioniert leider trotzdem nicht so ganz ...\n",
    "\n",
    "# conv1 layer\n",
    "model.add(Convolution2D(4, 3, 3, border_mode='same', activation='relu', input_shape=[img_size[0],img_size[1],3]))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "#2\n",
    "model.add(Convolution2D(4, 3, 3, border_mode='same', activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=2, strides=(2,2), border_mode='same'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "#3\n",
    "model.add(Convolution2D(8, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "#4\n",
    "model.add(Convolution2D(8, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "#5\n",
    "model.add(Convolution2D(8, 3, 3, border_mode='same', activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=2, strides=(2,2), border_mode='same'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "#6\n",
    "model.add(Convolution2D(16, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "#7\n",
    "model.add(Convolution2D(16, 3, 3, border_mode='same', activation='relu'))\n",
    "#model.add(UpSampling2D())\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "#8\n",
    "model.add(Convolution2D(8, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "#9\n",
    "model.add(Convolution2D(8, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "#10\n",
    "model.add(Convolution2D(8, 3, 3, border_mode='same', activation='relu'))\n",
    "#model.add(UpSampling2D())\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "#11\n",
    "model.add(Convolution2D(4, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "#12\n",
    "model.add(Convolution2D(1, 3, 3, border_mode='same', activation='relu'))\n",
    "\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 31s 31ms/step - loss: nan - acc: 8.7500e-05\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 28s 28ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 27s 27ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 28s 28ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 29s 29ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 29s 29ms/step - loss: nan - acc: 8.3984e-050s - loss: nan - acc: 8.16\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 30s 30ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 30s 30ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 36s 36ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 36s 36ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: nan - acc: 8.3984e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x655c74a8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator( train_generator, steps_per_epoch = 1000, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exc. 7.2 FCN mit Bottleneck\n",
    "\n",
    "Implementiere jetzt die Variante mit schrittweisem Down- und Upsampling, wie in der Vorlesung besprochen. Nutze dafür ein bestehendes Netzwerk (z.B. VGG16, https://keras.io/applications/#vgg16), entferne die FC-Schichten am Ende, und füge dann die Upsampling-Schichten hinzu. Wie in der vorigen Vorlesung zu Transfer Learning beschrieben, kannst Du jetzt nur den zweiten Teil trainieren und die Gewichte des ersten Teils \"einfrieren\".\n",
    "\n",
    "Stelle wie oben den Verlauf des Losses dar und wähle einige Beispielbilder aus dem Testset und zeige sie mit ihrer vorhergesagten Segmentierung an. (**BONUS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
